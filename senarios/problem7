http://arun-teaches-u-tech.blogspot.sg/p/problem-7.html

sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username retail_dba --password cloudera \
--table orders --target-dir /user/cloudera/problem7/prework --as-avrodatafile -m 1

hadoop fs -get /user/cloudera/problem7/prework

cp /etc/flume-ng/conf/flume-conf.properties.template problem7.conf
agent.sources = seqGenSrc
agent.channels = memoryChannel
agent.sinks = loggerSink

# For each one of the sources, the type is defined
agent.sources.seqGenSrc.type = avro
agent.sources.seqGenSrc.bind = localhost
agent.sources.seqGenSrc.port = 11112

# The channel can be defined as follows.
agent.sources.seqGenSrc.channels = memoryChannel

# Each sink's type must be defined
agent.sinks.loggerSink.type = hdfs
agent.sinks.loggerSink.hdfs.path = /user/cloudera/problem7/sink
agent.sinks.loggerSink.hdfs.fileSuffix = .avro
agent.sinks.loggerSink.hdfs.fileType = DataStream
agent.sinks.loggerSink.serializer = avro_event
agent.sinks.loggerSink.serializer.compressionCodec=snappy

#Specify the channel the sink should use
agent.sinks.loggerSink.channel = memoryChannel

# Each channel's type is defined.
agent.channels.memoryChannel.type = memory
agent.channels.memoryChannel.capacity = 10000
agent.channels.memoryChannel.transactionCapacity = 1000

flume-ng agent --name agent --conf-file problem7.conf
    INFO source.AvroSource: Avro source seqGenSrc started
flume-ng avro-client -H localhost -p 11112 -F /home/cloudera/prework/part-m-00000.avro

Wait asynchronous flume job completed
    INFO hdfs.HDFSEventSink: Writer callback called.


start_logs
tail -F /opt/gen_logs/logs/access.log
cp -f /etc/flume-ng/conf/flume-conf.properties.template problem7b.conf
agent.sources = seqGenSrc
agent.channels = memoryChannel
agent.sinks = loggerSink

agent.sources = seqGenSrc
agent.channels = memoryChannel
agent.sinks = loggerSink

# For each one of the sources, the type is defined
agent.sources.seqGenSrc.type = exec
agent.sources.seqGenSrc.command = tail -F /opt/gen_logs/logs/access.log

# The channel can be defined as follows.
agent.sources.seqGenSrc.channels = memoryChannel

# Each sink's type must be defined
agent.sinks.loggerSink.type = hdfs
agent.sinks.loggerSink.hdfs.path = /user/cloudera/problem7/step2
agent.sinks.loggerSink.hdfs.fileType = DataStream
agent.sinks.loggerSink.hdfs.fileSuffix = .log
agent.sinks.loggerSink.hdfs.writeFormat = Text

#Specify the channel the sink should use
agent.sinks.loggerSink.channel = memoryChannel

# Each channel's type is defined.
agent.channels.memoryChannel.type = memory
agent.channels.memoryChannel.capacity = 1000
agent.channels.memoryChannel.transactionCapacity = 200

flume-ng agent --name agent --conf-file problem7b.conf
hadoop fs -ls /user/cloudera/problem7/step2
5:42
